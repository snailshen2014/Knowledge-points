#           **分布式锁常见的解决方案**

## 一、分布式锁简介

在我们进行单机应用开发，涉及并发同步的时候，我们往往采用synchronized或者Lock的方式来解决多线程间的代码同步问题。但当我们的应用是分布式集群工作的情况下，那么就需要一种更加高级的锁机制，来处理种跨机器的进程之间的数据同步问题，这就是分布式锁。

## 二、常见的解决方案

* 基于数据库实现的分布式锁
* 基于Redis实现的分布式锁
* 基于Zookeeper实现的分布式锁

高并发下数据库锁性能太差，本文不做探究。仅针对Redis 和 Zookeeper 实现的分布式锁进行分析。实现一个分布式锁应该具备的特性：

* 锁服务高可用、高性能
* 锁数据一致性
* 死锁预防机制

Redis在锁时间限制和缓存一致性存在一定问题，Zookeeper在可靠性上强于Redis，只是效率相对较低，开发人员需要根据实际需求进行技术选型。

## 三、落地方案

#### 1、基于Redis的锁服务

基于Redis实现的分布式锁服务，思路简单直接，我们把锁数据存放在分布式环境中的一个或者多个节点上，所有需要获取锁的调用方都去访问此节点锁数据，从而实现对调用方的互斥。

##### 1.1、单实例锁服务

通过一个Redis实例实现分布式锁服务。

###### 1.1.1、加锁流程

基于Redis官网文档，对于一个尝试获取锁的操作，流程如下：

向Redis节点发送命令：

   SET(key=Lock_name,value=My_random_value) NX PX 30000

* My_random_value是由客户端生成的一串随机字符串，它保证在足够长的一段时间内客户端的所有获取锁的请求中都是唯一的，用于唯一标识锁持有方。
* NX表示只有Lock_name对应的key不存在时才能SET成功，这保证了只有第一个请求的客户端才能获取锁，而其它客户端在锁被释放前都无法获得锁。
* PX 30000,表示这个锁节点有一个30秒的自动过期时间（防止客户端异常时，锁没有被释放，其它客户端无法获取到锁，产生了死锁）

如果命令返回成功，则代表获取锁成功，否则获取锁失败。对于一个拥有锁的客户端，释放锁流程如下：

* GET (key=Lock_name)
* 如果查询回来的value和本身的My_random_value一致，则表示自己是锁的持有者，可以发起解锁操作，发送命令：DEL(key=Lock_name)

**注意：**这个释放锁的操作为了原子性通常会通过运行lua脚本在一个原子性操作中完成。

###### 1.1.2、锁安全性分析

* 锁数据一致性问题，Redis存在单点问题，如果实例挂了，锁服务不可用，如果采用Master/SLAVE架构时，master挂掉slave来不及同步数据，锁数据失效，锁服务不可用。

* 简单的依靠过期时间控制锁，必须保证客户端在这个时间内完成业务，否则会有资源冲突。

  比如客户端1获取锁成功；客户端1在某个操作上阻塞了很长时间；过期时间到，锁自动释放；客户端2获取到了对应的同一个资源的锁；客户端1恢复过来认为自己还持有锁，继续操作同一个资源。导致资源冲突

* 释放锁时必须要释放自己的锁，通过lua脚本来控制，保证GET,DEL原子性

##### 1.2、多实例锁服务-RedLock

为了解决锁数据的一致性问题,Redis作者antirez基于分布式环境下提出了一种更高级的分布式锁的实现方式：**Redlock**。

antirez提出的redlock算法大概是这样的：

在Redis的分布式环境中，我们假设有N个Redis master。这些节点**完全互相独立，不存在主从复制或者其他集群协调机制**。我们确保将在N个实例上使用与在Redis单实例下相同方法获取和释放锁。现在我们假设有5个Redis master节点(官方文档里将N设置成5，其实大等于3就行)，同时我们需要在5台服务器上面运行这些Redis实例，这样保证他们不会同时都宕掉。

###### 1.2.1、加锁流程

为了取到锁，客户端应该执行以下操作:

- 获取当前Unix时间，以毫秒为单位。
- 依次尝试项N个Redis实例节点执行获取锁的操作（其实可以并发获取），当向Redis请求获取锁时，客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试去另外一个Redis实例请求获取锁。
- 计算整个获取锁的过程总共消耗了多长时间，计算方法时用当前时间减去第一步记录的时间，如果客户端从大多数Redis节点>=N/2+1,成功获取到了锁，并且获取锁的总耗时没有超过锁的有效时间，那么这时客户端才认为最终获取锁成功；否则最终获取锁失败。
- 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。
- 如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在**所有的Redis实例上进行解锁**（即便某些Redis实例根本就没有加锁成功，防止某些节点获取到锁但是客户端没有得到响应而导致接下来的一段时间不能被重新获取锁）。

RedLock的java落地实现可以参考redisson

https://github.com/redisson/redisson/



###### 1.2.2、锁安全性分析

 RedLock算法的最核心也是最有价值之处，是引入了多数派的思想，来解决单点故障对数据安全性和服务可用性的影响。由于加锁成功需要所有节点中的多数统一，因此只要集群中节点有一半能够提供服务时，服务的可用性就能够保障，同时对数据的一致性，只要对于一把锁数据，多数节点数据不丢失，那么锁就不可能被另外的调用方同时获得，所以锁的安全性可以得到保证。但是这个算法还存在如下两个问题：

* RedLock的安全性依旧强依赖于系统时间，会产生时间跳跃问题

  ```properties
  假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列：
  
  ​    客户端1从Redis节点A, B, C成功获取了锁（多数节点）
  
  ​    节点C上的时钟发生了向前跳跃，导致它上面维护的锁快速过期。 
  
  ​    客户端2从Redis节点C, D, E成功获取了同一个资源的锁（多数节点）。
  
  ​    客户端1和客户端2现在都认为自己持有了锁。     
  ```

* 缺乏数据丢失的识别机制和恢复机制

  ```properties
  假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列
  
  客户端1成功锁住了A,B,C获取锁成功
  
  节点C宕机，单客户端1在C的锁数据没有持久化下来，丢失了。
  
  节点C重启后，客户端2锁住了C,D,E获取锁成功
  
  客户端1和客户端2现在都认为自己持有了锁
  ```

  此类问题的本质是作为多数派的一个结点，数据丢失后（比如故障未落地、超时被清理），首先没有能够区分丢失了哪些数据的能力，其次还有没有恢复丢失数据的能力，这两种能力都缺失的情况下，数据节点继续正常的参与投票，从而导致数据一致性的破坏。RedLock也意识到了这个问题，所以其中有一个延迟重启（delayed restarts)的概念，就是一个节点宕机后不要立刻重启，而是等待一段时间在重启

  

#### 2、基于zookeeper的锁服务

##### 2.1、zookeeper的写数据的原理

Zookeeper在集群部署中，Zookeeper节点数量一般是奇数，且一定大等于3，写数据的流程如下：

- 在Client向Follwer发出一个写的请求
- Follwer把请求发送给Leader
- Leader接收到以后开始发起投票并通知Follwer进行投票
- Follwer把投票结果发送给Leader，**只要半数以上返回了ACK信息，就认为通过**
- Leader将结果汇总后如果需要写入，则开始写入同时把写入操作通知给Leader，然后commit;
- Follwer把请求结果返回给Client

还有一点，Zookeeper采取的是全局串行化操作。

##### 2.2、锁安全性分析

下面列出Redis集群下分布式锁可能存在的问题，判断其在Zookeeper集群下是否会存在：

**集群同步**

- client给Follwer写数据，可是Follwer却宕机了，会出现数据不一致问题么？不可能，这种时候，client建立节点失败，根本获取不到锁。
- client给Follwer写数据，Follwer将请求转发给Leader，Leader宕机了，会出现不一致的问题么？不可能，这种时候，Zookeeper会选取新的leader，继续上面的提到的写流程。

总之，采用Zookeeper作为分布式锁，你要么就获取不到锁，一旦获取到了，必定节点的数据是一致的，不会出现redis那种异步同步导致数据丢失的问题。
**时间跳跃问题**
Zookeeper不依赖全局时间，不存在该问题。
**超时导致锁失效问题**
Zookeeper不依赖有效时间，不存在该问题。


zookeeper分布式锁的缺点：
https://www.jianshu.com/p/8ee125d57aad

基于zookeeper的分布式锁服务，java落地实现参考：

https://github.com/apache/curator



  









